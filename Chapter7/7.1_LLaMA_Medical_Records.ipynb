{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
      "Requirement already satisfied: torch in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (2.6.0+cu126)\n",
      "Requirement already satisfied: torchvision in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (0.21.0+cu126)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (2.6.0+cu126)\n",
      "Requirement already satisfied: filelock in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from torch) (78.0.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from torchvision) (2.2.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: transformers in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (4.50.0)\n",
      "Requirement already satisfied: peft in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (0.15.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: accelerate in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: bitsandbytes in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (0.45.3)\n",
      "Requirement already satisfied: trl in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (0.16.0)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (8.1.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from transformers) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from peft) (2.6.0+cu126)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from datasets) (3.11.14)\n",
      "Requirement already satisfied: rich in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from trl) (13.9.4)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from ipywidgets) (9.0.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: networkx in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from torch>=1.13.0->peft) (78.0.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from rich->trl) (3.0.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\inhag\\.virtualenvs\\chapter7-ohihpsxv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "# Llama 모델 LoRA 파인튜닝 예제\n",
    "# 필요한 라이브러리 설치\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n",
    "!pip install transformers peft datasets accelerate bitsandbytes trl ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine Tunning 활용 모델 다운로드 (활용 모델 Gemma 3 - 4B)\n",
    "\n",
    "#### User token 발행\n",
    "1. Google에서 개발한 Gemma 3 모델 (250312 release)을 다운로드 하기 위해 huggingface 웹사이트에 가입합니다. [https://huggingface.co/](https://huggingface.co/)\n",
    "2. 우측 상단 동그란 프로필 메뉴 -> Settings -> Access Tokens -> +Create new token 클릭 -> Read 권한 설정 후 API Token 복사 붙여넣기\n",
    "(image/image_7-1-1_HuggingFace 토큰 발급 방법.png)\n",
    "3. 단 User Token은 공유하지 않습니다. (아래 토큰은 예시)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c0208fe2db046c1bb5b7b5911306d57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\inhag\\\\DoctorCode-Workbook\\\\Chapter7\\\\gemma-3-4b-it'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 필요 모델 (Gemma 3 - 4B) 다운로드 (방법 3가지)\n",
    "\n",
    "# 1. huggingface_hub 라이브러리 사용\n",
    "from huggingface_hub import snapshot_download\n",
    "from huggingface_hub import login\n",
    "\n",
    "# # 방법 1: 대화형 로그인 (프롬프트에 토큰 입력)\n",
    "# login()\n",
    "\n",
    "# 방법 2: 코드에 토큰 직접 입력 (보안상 권장되지 않음)\n",
    "login(token=\"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\")\n",
    "\n",
    "# 모델 전체 다운로드\n",
    "model_id = \"google/gemma-3-4b-it\"  # 예시 모델명\n",
    "local_dir = \"./gemma-3-4b-it\"\n",
    "snapshot_download(repo_id=model_id, local_dir=local_dir)\n",
    "\n",
    "# # 2. transformers 라이브러리 사용하여 다운로드\n",
    "# from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# # 모델과 토크나이저 로드 (처음에는 다운로드됨)\n",
    "# model_name = \"google/gemma-3-4b-it\"  # 예시 모델명\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# # 특정 로컬 디렉토리에 저장하기\n",
    "# model_path = \"./llm_model\"\n",
    "# tokenizer.save_pretrained(model_path)\n",
    "# model.save_pretrained(model_path)\n",
    "\n",
    "# 2. git-lfs clone을 통한 다운로드\n",
    "# !git-lfs clone https://huggingface.co/google/gemma-3-4b-it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 모델 및 토크나이저 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed092a0261bb4721b89dbf25aa26a38d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 양자화 설정 (메모리 효율성 향상)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "model_path = 'gemma-3-4b-it'\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# 모델 로드 (양자화 적용)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    token=True\n",
    ")\n",
    "\n",
    "# kbit 학습을 위한 모델 준비\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. LoRA 설정 및 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 설정\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                   # 행렬의 랭크\n",
    "    lora_alpha=32,          # 스케일링 파라미터\n",
    "    lora_dropout=0.05,      # 드롭아웃 비율\n",
    "    bias=\"none\",           \n",
    "    task_type=\"CAUSAL_LM\",  # 태스크 유형\n",
    "    target_modules=[        # LoRA를 적용할 모듈\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 모델에 LoRA 적용\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 데이터셋 준비\n",
    "\n",
    "#### Alpaca 데이터셋 구조\n",
    "\n",
    "Alpaca 데이터셋은 Stanford에서 만든 지시 튜닝용 데이터셋으로, 다음과 같은 필드를 포함합니다:\n",
    "\n",
    "- instruction: 모델에게 주어지는 작업 지시사항\n",
    "- input: 작업 수행에 필요한 추가 입력 정보 (선택적)\n",
    "- output: 모델이 생성해야 하는 기대 출력\n",
    "\n",
    "샘플 예시\n",
    "\n",
    "{\n",
    "  \"instruction\": \"텍스트에서 긍정적인 감정을 찾아내세요.\",\n",
    "  \"input\": \"오늘은 날씨가 좋아서 기분이 좋습니다. 하지만 교통체증 때문에 회사에 늦었어요.\",\n",
    "  \"output\": \"긍정적인 감정: '날씨가 좋아서 기분이 좋습니다'에서 '기분이 좋다'라는 긍정적인 감정이 표현되어 있습니다.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시 데이터셋 로드 (실제 사용 시 자신의 데이터셋을 사용)\n",
    "# 여기서는 예시로 Alpaca 데이터셋 사용\n",
    "dataset = load_dataset(\"tatsu-lab/alpaca\")\n",
    "\n",
    "# 데이터 전처리 함수\n",
    "def formatting_func(examples):\n",
    "    text = []\n",
    "    for instruction, input_text, output in zip(examples['instruction'], examples['input'], examples['output']):\n",
    "        if input_text:\n",
    "            prompt = f\"### 질문: {instruction}\\n\\n### 입력:\\n{input_text}\\n\\n### 응답:\\n\"\n",
    "        else:\n",
    "            prompt = f\"### 질문: {instruction}\\n\\n### 응답:\\n\"\n",
    "        text.append(prompt + output)\n",
    "    return {\"text\": text}\n",
    "\n",
    "# 데이터셋 전처리\n",
    "formatted_dataset = dataset.map(formatting_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 구조: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'input', 'output', 'text'],\n",
      "        num_rows: 52002\n",
      "    })\n",
      "})\n",
      "학습 데이터 크기: 52002\n",
      "컬럼 목록: ['instruction', 'input', 'output', 'text']\n",
      "\n",
      "처음 3개 샘플:\n",
      "\n",
      "샘플 1:\n",
      "Instruction: Give three tips for staying healthy.\n",
      "Input: \n",
      "Output: 1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
      "2. Exercise regularly to keep your body active and strong. \n",
      "3. Get enough sleep and maintain a consistent sleep schedule.\n",
      "\n",
      "샘플 2:\n",
      "Instruction: What are the three primary colors?\n",
      "Input: \n",
      "Output: The three primary colors are red, blue, and yellow.\n",
      "\n",
      "샘플 3:\n",
      "Instruction: Describe the structure of an atom.\n",
      "Input: \n",
      "Output: An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.\n",
      "\n",
      "포맷팅된 예시:\n",
      "### 질문: Give three tips for staying healthy.\n",
      "\n",
      "### 응답:\n",
      "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
      "2. Exercise regularly to keep your body active and strong. \n",
      "3. Get enough sleep and maintain a consistent sleep schedule.\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 정보 출력\n",
    "print(f\"데이터셋 구조: {dataset}\")\n",
    "print(f\"학습 데이터 크기: {len(dataset['train'])}\")\n",
    "print(f\"컬럼 목록: {dataset['train'].column_names}\")\n",
    "\n",
    "# 처음 3개 샘플 출력\n",
    "print(\"\\n처음 3개 샘플:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\n샘플 {i+1}:\")\n",
    "    print(f\"Instruction: {dataset['train'][i]['instruction']}\")\n",
    "    print(f\"Input: {dataset['train'][i]['input']}\")\n",
    "    print(f\"Output: {dataset['train'][i]['output']}\")\n",
    "\n",
    "# formatting_func 적용 예시 보기\n",
    "def formatting_func(examples):\n",
    "    text = []\n",
    "    for instruction, input_text, output in zip(examples['instruction'], examples['input'], examples['output']):\n",
    "        if input_text:\n",
    "            prompt = f\"### 질문: {instruction}\\n\\n### 입력:\\n{input_text}\\n\\n### 응답:\\n\"\n",
    "        else:\n",
    "            prompt = f\"### 질문: {instruction}\\n\\n### 응답:\\n\"\n",
    "        text.append(prompt + output)\n",
    "    return {\"text\": text}\n",
    "\n",
    "# 예시 포맷팅 출력\n",
    "formatted_examples = formatting_func({\n",
    "    'instruction': [dataset['train'][0]['instruction']],\n",
    "    'input': [dataset['train'][0]['input']],\n",
    "    'output': [dataset['train'][0]['output']]\n",
    "})\n",
    "\n",
    "print(\"\\n포맷팅된 예시:\")\n",
    "print(formatted_examples['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 학습 파라미터 설정 및 학습 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "It is strongly recommended to train Gemma3 models with the `eager` attention implementation instead of `sdpa`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "c:\\Users\\inhag\\.virtualenvs\\Chapter7-ohihPSxV\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='3250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   5/3250 06:20 < 114:24:50, 0.01 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 학습 파라미터 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora-output\",           # 출력 디렉토리\n",
    "    per_device_train_batch_size=4,        # 배치 크기\n",
    "    gradient_accumulation_steps=4,        # 그래디언트 누적 단계\n",
    "    learning_rate=2e-4,                   # 학습률\n",
    "    num_train_epochs=1,                   # 학습 에폭 수\n",
    "    warmup_ratio=0.03,                    # 워밍업 비율\n",
    "    logging_steps=10,                     # 로깅 단계\n",
    "    save_strategy=\"epoch\",                # 저장 전략\n",
    "    optim=\"paged_adamw_32bit\",            # 옵티마이저\n",
    "    fp16=True,                            # 16비트 연산 사용\n",
    ")\n",
    "\n",
    "# SFTTrainer 설정\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=formatted_dataset[\"train\"],\n",
    "    args=training_args,\n",
    "    peft_config=lora_config,  # LoRA 설정 추가\n",
    ")\n",
    "\n",
    "# 학습 실행\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파인튜닝된 모델 저장\n",
    "model.save_pretrained(\"./lora-finetuned\")\n",
    "tokenizer.save_pretrained(\"./lora-finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. 파인튜닝된 모델 추론 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파인튜닝된 모델로 추론하기\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# 모델 구성 로드\n",
    "config = PeftConfig.from_pretrained(\"./llama-lora-finetuned\")\n",
    "\n",
    "# 기본 모델 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    device_map=\"auto\",\n",
    "    token=True\n",
    ")\n",
    "\n",
    "# LoRA 가중치 로드\n",
    "model = PeftModel.from_pretrained(model, \"./llama-lora-finetuned\")\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# 추론 함수\n",
    "def generate_response(prompt, max_length=512):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=max_length,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# 테스트\n",
    "test_prompt = \"### 질문: 딥러닝 모델 학습 시 과적합을 방지하는 방법을 설명해주세요.\\n\\n### 응답:\\n\"\n",
    "response = generate_response(test_prompt)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Chapter7-ohihPSxV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
